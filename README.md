# Codebase RAG - Asistente Inteligente de C√≥digo

Un sistema de Retrieval-Augmented Generation (RAG) dise√±ado para ayudarte a consultar y comprender bases de c√≥digo usando lenguaje natural. Esta herramienta indexa archivos de c√≥digo en una base de datos vectorial y habilita b√∫squeda sem√°ntica y respuestas impulsadas por IA sobre tu codebase.

## Descripci√≥n del Proyecto

Codebase RAG es un asistente de codebase que permite al usuario preguntar al agente usando lenguaje natural y obtener referencias y explicaciones sobre el c√≥digo. El proyecto est√° dise√±ado para usarse con codebases relativamente grandes, ayudando a los desarrolladores a tener un conocimiento m√°s completo y basado en la totalidad del c√≥digo sobre los repositorios donde se encuentren trabajando.

### Caracter√≠sticas Principales

- **Soporte multi-lenguaje**: Maneja m√°s de 30 lenguajes de programaci√≥n y tipos de archivo incluyendo Python, JavaScript, TypeScript, Java, C/C++, Go, Rust y m√°s
- **Parseo inteligente de c√≥digo**: Utiliza parsers espec√≠ficos por lenguaje para extraer fragmentos de c√≥digo significativos
- **Recuperaci√≥n basada en vectores**: Aprovecha sentence transformers para b√∫squeda por similaridad sem√°ntica
- **Respuestas impulsadas por IA**: Integraci√≥n con modelos GPT de OpenAI para proporcionar respuestas en lenguaje natural
- **Soporte de archivos ZIP**: Procesa codebases completos empaquetados como archivos zip
- **Almacenamiento persistente**: Usa ChromaDB para almacenamiento y recuperaci√≥n eficiente de documentos

### Input del Sistema

Texto en lenguaje natural sobre la codebase. Por ejemplo:
- "¬øC√≥mo funciona el sistema de autenticaci√≥n?"
- "¬øQu√© tests le faltan a esta funci√≥n?"
- "¬øCu√°l es la relaci√≥n entre main.py y models.py?"
- "Explicame la arquitectura del proyecto"

### Output del Sistema

Snippets de c√≥digo (con sus archivos de referencia) acompa√±ados de explicaciones contextuales que responden a las preguntas del usuario.

## Arquitectura

El sistema consta de tres componentes principales:

1. **Detecci√≥n de Lenguaje** ([src/utils/language_detector.py](src/utils/language_detector.py)): Identifica tipos de archivo y filtra lenguajes soportados
2. **Fragmentaci√≥n y Parseo de Texto** ([src/text_splitter.py](src/text_splitter.py)): Extrae y fragmenta c√≥digo de archivos fuente usando parsers conscientes del lenguaje
3. **Almacenamiento Vectorial y RAG** ([src/inserter.py](src/inserter.py)): Gestiona colecciones de ChromaDB, realiza b√∫squeda por similaridad y genera respuestas de IA

### Pipeline RAG

1. **Carga de Datos**: Los archivos fuente se cargan desde archivos ZIP
2. **Preprocesamiento**:
   - Detecci√≥n autom√°tica del lenguaje de programaci√≥n
   - Filtrado de archivos soportados
   - Parseo espec√≠fico por lenguaje usando tree-sitter
3. **Text Splitting**: Fragmentaci√≥n inteligente del c√≥digo respetando la estructura del lenguaje
4. **Generaci√≥n de Embeddings**: Conversi√≥n de fragmentos de c√≥digo a vectores usando SentenceTransformers (BERT)
5. **Almacenamiento**: Embeddings y metadata se almacenan en ChromaDB
6. **Retrieval**: B√∫squeda de los K fragmentos m√°s similares sem√°nticamente a la consulta
7. **Generaci√≥n**: Los fragmentos relevantes se pasan a GPT junto con la pregunta para generar una respuesta contextual

## Instalaci√≥n

### Prerequisitos

- Python 3.11 o superior
- OpenAI API key o Gemini API key

### Configuraci√≥n

1. Clona el repositorio:
```bash
git clone <repository-url>
cd codebase-rag
```

2. Instala las dependencias:
```bash
pip install -r requirements.txt
```

3. Crea un archivo `.env` en la ra√≠z del proyecto y agrega tu API key de OpenAI:
```bash
OPENAI_API_KEY=tu_api_key_aqui
```

## Gu√≠a de Uso

El sistema soporta **dos modos de operaci√≥n**:

1. **Modo Chat Interactivo** (nuevo): Conversaciones multi-turno con contexto mantenido
2. **Modo Single-Shot**: Una pregunta, una respuesta y salida (modo original)

---

### üí¨ Modo Chat Interactivo (Recomendado)

Modo conversacional que mantiene el contexto entre preguntas. Ideal para exploraci√≥n de c√≥digo.

#### Desde archivo ZIP:
```bash
python main.py -z <ruta_al_zip> --config configs/chat_openai.json
```

**Ejemplo:**
```bash
python main.py -z test_data --config configs/chat_openai.json
```

**Sesi√≥n ejemplo:**
```
====================================
  MODO CHAT INTERACTIVO
====================================

Config: chat_openai
Modelo: gpt-4o-mini

Comandos: /help, /clear, /exit

====================================

Q: ¬øC√≥mo se crea un usuario?
Buscando...
A: Para crear un usuario, instancia la clase Usuario...

Q: ¬øY c√≥mo le asigno un rol?
Buscando...
A: (Recuerda el contexto anterior sobre usuarios)
Los roles est√°n definidos en config.py...

Q: /clear
Conversaci√≥n reiniciada

Q: /exit
Hasta pronto!
```

#### Desde repositorio de GitHub:
```bash
python main.py -g <url_github> --config configs/chat_openai.json
```

#### Comandos disponibles en modo chat:

| Comando | Descripci√≥n |
|---------|-------------|
| `/help` | Mostrar ayuda de comandos |
| `/clear` | Reiniciar conversaci√≥n (nuevo contexto) |
| `/exit` | Salir del chat |

#### Configuraciones para modo chat:

El sistema incluye configuraciones optimizadas para chat con OpenAI (soporta historial conversacional):

**1. `chat_openai.json` - Balanceada** (Recomendada)
```bash
python main.py -z proyecto.zip --config configs/chat_openai.json
```
- Modelo: `gpt-4o-mini` (balance calidad/costo)
- Reranking: Cross-Encoder habilitado
- Historial: ‚úÖ Completo

**2. `chat_openai_fast.json` - R√°pida y econ√≥mica**
```bash
python main.py -z proyecto.zip --config configs/chat_openai_fast.json
```
- Modelo: `gpt-3.5-turbo` (m√°s barato)
- Sin reranking (m√°s r√°pido)
- Historial: ‚úÖ Completo

**3. `chat_openai_premium.json` - M√°xima calidad**
```bash
python main.py -z proyecto.zip --config configs/chat_openai_premium.json
```
- Modelo: `gpt-4o` (el mejor)
- Reranking agresivo (25‚Üí8 docs)
- Historial: ‚úÖ Completo

**Nota:** Solo OpenAI soporta historial conversacional. Con Gemini cada pregunta es independiente.

Ver documentaci√≥n completa: [configs/CHAT_CONFIGS.md](configs/CHAT_CONFIGS.md)

---

### üìù Modo Single-Shot

Una pregunta, una respuesta. √ötil para scripts y automatizaci√≥n.

#### Desde archivo ZIP:
```bash
python main.py -z <ruta_al_zip> -p "pregunta sobre el c√≥digo"
```

**Ejemplo:**
```bash
python main.py -z test_codebase.zip -p "¬øC√≥mo funciona la autenticaci√≥n?"
```

#### Desde repositorio de GitHub:
```bash
python main.py -g <url_github> -p "pregunta sobre el c√≥digo"
```

**Ejemplo:**
```bash
python main.py -g https://github.com/usuario/repo -p "¬øQu√© hace este c√≥digo?"
```

#### Opciones adicionales:

- `-c, --collection-name`: Nombre de la colecci√≥n (default: "codeRAG")
- `--config`: Ruta al archivo de configuraci√≥n JSON (opcional)
- `-v, --verbose`: Muestra logs detallados del proceso RAG

**Ejemplo con todas las opciones:**
```bash
python main.py -z mi_proyecto.zip -p "¬øC√≥mo se crea un usuario?" -c mi_coleccion --config configs/optimal.json -v
```

### Modo Verbose

El flag `-v` o `--verbose` activa logs detallados que muestran:

1. **Fragmentos recuperados**: Preview de los 5 fragmentos m√°s relevantes encontrados
2. **Informaci√≥n del contexto**: Longitud total y n√∫mero de fragmentos usados
3. **Detalles de generaci√≥n**: Modelo usado, tokens aproximados y tokens totales consumidos

**Sin verbose:**
```bash
python main.py -z test_codebase.zip -p "¬øQu√© hace este c√≥digo?"
# Solo muestra: Fases principales ‚Üí Respuesta final
```

**Con verbose:**
```bash
python main.py -z test_codebase.zip -p "¬øQu√© hace este c√≥digo?" -v
# Muestra: Fases + Fragmentos recuperados + M√©tricas + Respuesta
```

El modo verbose es √∫til para:
- Entender qu√© fragmentos de c√≥digo est√° usando el RAG
- Debuggear respuestas inesperadas
- Evaluar la calidad de la recuperaci√≥n
- Ver el consumo de tokens de la API

### Sistema de Configuraci√≥n JSON

El sistema permite configurar todos los par√°metros del RAG mediante archivos JSON, facilitando la experimentaci√≥n con diferentes configuraciones sin modificar c√≥digo.

#### Archivos de Configuraci√≥n Incluidos

El proyecto incluye 4 configuraciones predefinidas en la carpeta `configs/`:

**1. `default.json` - Configuraci√≥n balanceada**
```bash
python main.py -z code.zip -p "pregunta" --config configs/default.json
```
- Modelo: `gpt-4.1-nano`
- K documentos: 5
- Temperature: 0.1
- Uso: General, balance entre calidad y costo

**2. `optimal.json` - M√°xima calidad**
```bash
python main.py -z code.zip -p "pregunta" --config configs/optimal.json
```
- Modelo: `gpt-4o`
- K documentos: 8
- Temperature: 0.05
- Embeddings: `all-mpnet-base-v2` (mejor calidad)
- Uso: An√°lisis cr√≠tico, m√°xima precisi√≥n

**3. `fast.json` - Respuestas r√°pidas**
```bash
python main.py -z code.zip -p "pregunta" --config configs/fast.json
```
- Modelo: `gpt-3.5-turbo`
- K documentos: 3
- Max tokens: 500
- Uso: Consultas r√°pidas, desarrollo

**4. `detailed.json` - Explicaciones exhaustivas**
```bash
python main.py -z code.zip -p "pregunta" --config configs/detailed.json
```
- Modelo: `gpt-4.1-nano`
- K documentos: 7
- Max tokens: 1500
- Uso: An√°lisis profundo, documentaci√≥n

#### Estructura de un Archivo de Configuraci√≥n

```json
{
  "name": "mi_config",
  "description": "Descripci√≥n de la configuraci√≥n",

  "prompt": {
    "template": "optimal",
    "include_metadata": true,
    "max_context_length": 8000
  },

  "model": {
    "name": "gpt-4.1-nano",
    "temperature": 0.1,
    "max_tokens": null,
    "top_p": 1.0
  },

  "retrieval": {
    "k_documents": 5,
    "include_metadata": true,
    "similarity_threshold": null
  },

  "text_splitting": {
    "parser_threshold": 3
  },

  "embeddings": {
    "model_name": "sentence-transformers/all-MiniLM-L6-v2",
    "device": "cpu",
    "normalize_embeddings": true,
    "distance_function": "l2"
  }
}
```

#### Par√°metros Configurables

**Prompt:**
- `template`: Plantilla de prompt (`default`, `optimal`, `detailed`, `concise`, `spanish`, `beginner_friendly`)
- `max_context_length`: L√≠mite de caracteres del contexto

**Model (OpenAI):**
- `name`: Modelo a usar (`gpt-4o`, `gpt-4.1-nano`, `gpt-3.5-turbo`)
- `temperature`: Controla aleatoriedad (0.0-2.0, menor = m√°s determin√≠stico)
- `max_tokens`: L√≠mite de tokens en respuesta (null = sin l√≠mite)
- `top_p`: Nucleus sampling (0.0-1.0)

**Retrieval:**
- `k_documents`: N√∫mero de fragmentos a recuperar
- `similarity_threshold`: Filtro de similitud m√≠nima (null = sin filtro)

**Text Splitting:**
- `parser_threshold`: Tama√±o m√≠nimo de fragmentos de c√≥digo

**Embeddings:**
- `model_name`: Modelo de sentence-transformers
  - `all-MiniLM-L6-v2`: R√°pido, buena calidad (default)
  - `all-mpnet-base-v2`: Mejor calidad, m√°s lento
- `device`: `cpu` o `cuda` (para GPU)
- `distance_function`:
  - `l2`: Distancia euclideana. El valor default
  - `cosine`: Similitud coseno
  - `ip`: Producto escalar

#### Crear Tu Propia Configuraci√≥n

1. Copia una configuraci√≥n existente:
```bash
cp configs/default.json configs/mi_config.json
```

2. Modifica los par√°metros seg√∫n tus necesidades

3. √ösala con el flag `--config`:
```bash
python main.py -z code.zip -p "pregunta" --config configs/mi_config.json
```

#### Uso Program√°tico con Configs

```python
from src.config_loader import RAGConfig
from src.inserter import ChromaCollection

# Cargar configuraci√≥n
config = RAGConfig.from_json("configs/optimal.json")

# Crear colecci√≥n con la config
collection = ChromaCollection("mi_proyecto", config=config)

# El RAG usar√° autom√°ticamente todos los par√°metros de la config
respuesta = collection.rag("¬øC√≥mo funciona esto?")
```

#### Trade-offs de Configuraci√≥n

| Par√°metro | ‚¨ÜÔ∏è Aumentar | ‚¨áÔ∏è Disminuir |
|-----------|------------|-------------|
| **k_documents** | M√°s contexto, m√°s costo | M√°s r√°pido, menos contexto |
| **temperature** | M√°s creativo, menos preciso | M√°s determin√≠stico |
| **parser_threshold** | Fragmentos grandes | Fragmentos granulares |
| **max_tokens** | Respuestas largas, m√°s caro | Respuestas breves |

### Proceso completo

Cuando ejecutas el comando, el sistema:
1. Crea una colecci√≥n de ChromaDB llamada seg√∫n `-c` (default: "codeRAG")
2. Extraer y parsear todos los archivos soportados del zip
3. Insertar fragmentos de c√≥digo en la base de datos vectorial
4. Ejecutar una consulta de ejemplo (puede ser modificada en el script)

### Uso Program√°tico

#### 1. Detecci√≥n de Lenguaje

Identifica tipos de archivo antes de procesarlos:

```python
from src.utils.language_detector import detect_language, is_supported, filter_supported_files

# Detectar lenguaje desde la ruta del archivo
lenguaje = detect_language('src/main.py')  # Retorna: 'python'

# Verificar si un archivo es soportado
if is_supported('app.js'):
    print("¬°Archivo soportado!")

# Filtrar una lista de archivos
archivos = ['main.py', 'imagen.png', 'app.js', 'README.md']
archivos_codigo = filter_supported_files(archivos)
# Retorna: ['main.py', 'app.js', 'README.md']

# Obtener todas las extensiones soportadas
extensiones = get_supported_extensions()  # ['.py', '.js', '.md', ...]
```

#### 2. Parseo y Carga de Archivos

Parsea archivos individuales o archivos zip completos:

```python
from src.text_splitter import parse_file, load_from_zipfile

# Parsear un archivo individual
documentos = parse_file('src/main.py', parser_threshold=3)

# Cargar codebase completo desde zip
todos_los_docs = load_from_zipfile('codebase.zip')
```

#### 3. Trabajando con ChromaDB

Almacena y recupera fragmentos de c√≥digo:

```python
from src.inserter import ChromaCollection
from langchain_core.documents import Document

# Inicializar una colecci√≥n
coleccion = ChromaCollection('mi_proyecto')

# Insertar documentos
docs = [
    Document(page_content="def hola(): print('Hola')", metadata={"file": "main.py"}),
    Document(page_content="class Usuario: pass", metadata={"file": "models.py"})
]
coleccion.insert_docs(docs)

# Recuperar documentos similares
consulta = "c√≥mo definir una funci√≥n"
docs_similares, resultados = coleccion.retrieve_k_similar_docs([consulta], k=5)
print(docs_similares)
```

#### 4. Consultas RAG

Haz preguntas sobre tu codebase:

```python
from src.inserter import ChromaCollection

# Inicializar colecci√≥n
coleccion = ChromaCollection('mi_proyecto')

# Consultar la codebase

respuesta = coleccion.rag(pregunta, model="gpt-4o-mini")
print(respuesta)

# Ejemplos de preguntas avanzadas:
# - "¬øQu√© tests le faltan a la funci√≥n de login?"
# - "¬øCu√°l es el code coverage del proyecto?"
# - "Expl√≠came la relaci√≥n entre el archivo auth.py y database.py"
# - "¬øQu√© mejoras de seguridad recomiendas para este c√≥digo?"
```

### Lenguajes y Extensiones Soportados

El sistema soporta las siguientes extensiones de archivo:

**Lenguajes de Programaci√≥n:**
- Python: `.py`, `.pyw`, `.pyx`
- JavaScript/TypeScript: `.js`, `.mjs`, `.cjs`, `.jsx`, `.ts`, `.tsx`
- Java: `.java`
- C/C++: `.c`, `.h`, `.cpp`, `.cc`, `.cxx`, `.hpp`, `.hxx`
- Go: `.go`
- Rust: `.rs`
- Ruby: `.rb`
- PHP: `.php`
- C#: `.cs`
- Swift: `.swift`
- Kotlin: `.kt`, `.kts`
- Scala: `.scala`
- Haskell: `.hs`
- Lua: `.lua`
- LaTeX: `.tex`

**Documentaci√≥n y Datos:**
- Markdown: `.md`, `.markdown`
- JSON: `.json`
- YAML: `.yaml`, `.yml`
- TOML: `.toml`
- XML: `.xml`
- Texto: `.txt`, `.rst`
- Config: `.ini`, `.cfg`, `.conf`

**Web:**
- HTML: `.html`, `.htm`
- CSS: `.css`, `.scss`, `.sass`, `.less`

**Otros:**
- Shell scripts: `.sh`, `.bash`, `.zsh`
- SQL: `.sql`
- R: `.r`, `.R`

## Configuraci√≥n Avanzada

### Parser Threshold

El par√°metro `parser_threshold` (por defecto: 3) controla el tama√±o m√≠nimo de los fragmentos de c√≥digo. Valores m√°s bajos crean fragmentos m√°s peque√±os y granulares.

```python
documentos = parse_file('archivo.py', parser_threshold=5)  # Fragmentos m√°s grandes
```

### Modelo de Embeddings

El sistema usa SentenceTransformer para generar embeddings (basado en BERT). Para cambiar el modelo, modifica la l√≠nea 11 en [src/inserter.py](src/inserter.py:11):

```python
_embedding_function = SentenceTransformerEmbeddingFunction(model_name="nombre-de-tu-modelo")
```

### Modelo GPT

El modelo por defecto es `gpt-4o-mini`. Puedes especificar un modelo diferente al llamar al m√©todo `rag()`:

```python
respuesta = coleccion.rag(consulta, model="gpt-4")
```

## Estructura del Proyecto

```
codebase-rag/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ inserter.py              # Gesti√≥n de ChromaDB e implementaci√≥n RAG
‚îÇ   ‚îú‚îÄ‚îÄ text_splitter.py         # Parseo de archivos y text splitting
‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py         # Cargador de configuraciones JSON
‚îÇ   ‚îú‚îÄ‚îÄ prompt_loader.py         # Cargador de plantillas de prompts
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ language_detector.py # Utilidades de detecci√≥n de lenguaje
‚îú‚îÄ‚îÄ configs/                      # Archivos de configuraci√≥n JSON
‚îÇ   ‚îú‚îÄ‚îÄ default.json
‚îÇ   ‚îú‚îÄ‚îÄ optimal.json
‚îÇ   ‚îú‚îÄ‚îÄ fast.json
‚îÇ   ‚îî‚îÄ‚îÄ detailed.json
‚îú‚îÄ‚îÄ prompts/                      # Plantillas de prompts
‚îÇ   ‚îú‚îÄ‚îÄ default.txt
‚îÇ   ‚îú‚îÄ‚îÄ optimal.txt
‚îÇ   ‚îú‚îÄ‚îÄ detailed.txt
‚îÇ   ‚îú‚îÄ‚îÄ concise.txt
‚îÇ   ‚îú‚îÄ‚îÄ spanish.txt
‚îÇ   ‚îî‚îÄ‚îÄ beginner_friendly.txt
‚îú‚îÄ‚îÄ dataset/                      # Datasets de prueba
‚îÇ   ‚îú‚îÄ‚îÄ jam-py-v7-develop.zip    # Proyecto web (JS/Python) ~15MB
‚îÇ   ‚îú‚îÄ‚îÄ numpy-main.zip           # Librer√≠a cient√≠fica (Python/C) ~10MB
‚îÇ   ‚îî‚îÄ‚îÄ SMTP-Protos-main.zip     # Protocolo de red (C) ~200KB
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_language_detector.py
‚îú‚îÄ‚îÄ context/                      # Material de referencia del curso
‚îÇ   ‚îú‚îÄ‚îÄ propuesta.txt
‚îÇ   ‚îú‚îÄ‚îÄ DeepLearningTP2.pdf
‚îÇ   ‚îî‚îÄ‚îÄ [notebooks de referencia]
‚îú‚îÄ‚îÄ main.py                       # Script principal CLI
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env                          # Tus API keys (crear este archivo)
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## Datasets de Prueba

El directorio `dataset/` contiene varios proyectos de c√≥digo de diferentes tama√±os y lenguajes de programaci√≥n para probar y evaluar el sistema RAG:

### Proyectos Incluidos

1. **SMTP-Protos (main)** - ~200KB
   - Implementaci√≥n de protocolo de red
   - Lenguaje: C
   - Tama√±o: Peque√±o
   - Ideal para: Pruebas r√°pidas, c√≥digo de bajo nivel, protocolos livianos

2. **jam-py (v7-develop)** - ~15MB
   - Framework web full-stack
   - Lenguajes: JavaScript, Python
   - Tama√±o: Peque√±o-Mediano (archivos grandes)
   - Ideal para: Probar parseo multi-lenguaje, arquitecturas web

3. **NumPy (main)** - ~10MB
   - Librer√≠a cient√≠fica de Python
   - Lenguajes: Python, C
   - Tama√±o: Mediano-Grande
   - Ideal para: Integraci√≥n Python/C, documentaci√≥n t√©cnica, c√≥digo cient√≠fico complejo

### Uso de los Datasets

Estos proyectos permiten evaluar el sistema en diferentes escenarios:
- **Variedad de lenguajes**: C, JavaScript, Python
- **Diferentes tama√±os**: Desde peque√±o (~200KB) hasta mediano-grande (~10MB)
- **Diversos dominios**: Protocolos de red, aplicaciones web, computaci√≥n cient√≠fica
- **Complejidad variada**: Protocolos simples vs librer√≠as cient√≠ficas complejas

**Ejemplo de uso:**
```bash
# Proyecto peque√±o (C)
python main.py -z dataset/SMTP-Protos-main.zip -p "¬øC√≥mo funciona el protocolo?"

# Proyecto peque√±o-mediano (JS/Python)
python main.py -z dataset/jam-py-v7-develop.zip -p "Explicame la arquitectura del framework"

# Proyecto mediano-grande (Python/C)
python main.py -z dataset/numpy-main.zip -p "¬øQu√© hace la funci√≥n array?"
```

## Desaf√≠os T√©cnicos

Los principales desaf√≠os t√©cnicos abordados en este proyecto incluyen:

1. **Parseo de C√≥digo Multi-lenguaje**: Implementaci√≥n de detecci√≥n autom√°tica y parseo espec√≠fico por lenguaje usando tree-sitter
2. **Fragmentaci√≥n Inteligente**: Divisi√≥n del contenido respetando la estructura y sintaxis de cada lenguaje
3. **Recuperaci√≥n y Rankeo Eficiente**: Uso de embeddings sem√°nticos para encontrar los fragmentos m√°s relevantes
4. **Referenciado a las Fuentes**: Mantener metadata precisa de archivos y ubicaciones para cada fragmento

## C√≥mo Funciona

1. **Procesamiento de Archivos**: El sistema identifica archivos soportados usando el detector de lenguaje
2. **Parseo**: Los parsers espec√≠ficos por lenguaje extraen segmentos de c√≥digo significativos
3. **Embedding**: Los fragmentos de c√≥digo se convierten a vectores usando sentence transformers
4. **Almacenamiento**: Los embeddings y metadata se almacenan en ChromaDB
5. **Retrieval**: Cuando haces una pregunta, el sistema encuentra los fragmentos m√°s sem√°nticamente similares
6. **Generaci√≥n**: Los fragmentos relevantes se pasan a GPT junto con tu pregunta para generar una respuesta contextual

## Testing

Ejecuta los tests del detector de lenguaje:

```bash
python tests/test_language_detector.py
```

Este script prueba todas las funcionalidades del m√≥dulo de detecci√≥n de lenguaje, incluyendo:
- Detecci√≥n de lenguaje por extensi√≥n
- Validaci√≥n de archivos soportados
- Filtrado de listas de archivos
- Obtenci√≥n de extensiones y lenguajes soportados

## Dependencias Principales

- `chromadb`: Base de datos vectorial para almacenamiento de documentos
- `openai`: Integraci√≥n con modelos GPT
- `langchain` y `langchain-community`: Procesamiento de documentos y text splitting
- `sentence-transformers`: Generaci√≥n de embeddings
- `tree-sitter` y `tree-sitter-languages`: Parseo consciente del lenguaje para c√≥digo
- `python-dotenv`: Gesti√≥n de variables de entorno

Ver [requirements.txt](requirements.txt) para la lista completa.

## Limitaciones Actuales

- Requiere OpenAI O Gemini API key (implica costos por uso de API)
- Los archivos binarios e im√°genes no son soportados
- El rendimiento depende del tama√±o de la codebase y el tama√±o de fragmento elegido
- La calidad de las respuestas depende de la relevancia de los fragmentos recuperados

## Evaluaci√≥n con RAGAS

El sistema incluye un m√≥dulo completo de evaluaci√≥n basado en [RAGAS](https://docs.ragas.io/) (Retrieval-Augmented Generation Assessment) para medir la calidad del sistema RAG.

### M√©tricas Disponibles

El sistema soporta las siguientes m√©tricas de RAGAS:

- **Faithfulness**: Eval√∫a si la respuesta generada se basa fielmente en el contexto recuperado (sin alucinaciones)
- **Answer Relevancy**: Mide qu√© tan relevante es la respuesta para la pregunta del usuario
- **Context Precision**: Eval√∫a si el contexto recuperado es preciso y relevante
- **Context Recall**: Mide qu√© tan completo es el contexto recuperado comparado con la respuesta ideal

### M√©tricas que Requieren Ground Truth (Referencias)

**IMPORTANTE**: Algunas m√©tricas requieren respuestas de referencia (ground truth):
- `context_precision` ‚úì requiere referencias
- `context_recall` ‚úì requiere referencias
- `faithfulness` ‚úó NO requiere referencias
- `answer_relevancy` ‚úó NO requiere referencias

### Evaluaci√≥n R√°pida

Evaluar tu sistema RAG en 3 pasos:

```bash
# 1. Preparar dataset de prueba (ver data/evaluation/test_queries.json)
# 2. Ejecutar evaluaci√≥n
python evaluate.py \
  --collection-name codeRAG \
  --test-dataset data/evaluation/test_queries.json \
  --metrics faithfulness,answer_relevancy

# 3. Ver resultados en results/evaluation_results.csv
```

### Uso Completo del Script de Evaluaci√≥n

**Evaluar con dataset existente:**
```bash
python evaluate.py \
  --collection-name codeRAG \
  --test-dataset data/evaluation/test_queries.json \
  --eval-config configs/evaluation.json \
  --output results/eval_2025_01_18.csv \
  --verbose
```

**Auto-generar queries de prueba:**
```bash
python evaluate.py \
  --collection-name codeRAG \
  --auto-generate 20 \
  --metrics faithfulness,answer_relevancy \
  --save-dataset data/evaluation/my_test_queries.json
```

**Generar referencias autom√°ticamente:**
```bash
python evaluate.py \
  --collection-name codeRAG \
  --test-dataset data/evaluation/code_specific_queries.json \
  --generate-references \
  --save-dataset data/evaluation/queries_with_refs.json
```

‚ö†Ô∏è **Nota**: Las referencias auto-generadas deben ser revisadas manualmente para garantizar calidad.

**Curaci√≥ninteractiva de referencias:**
```bash
python evaluate.py \
  --collection-name codeRAG \
  --test-dataset data/evaluation/test_queries.json \
  --interactive-curation
```

### Uso Program√°tico

```python
from src.evaluation import RAGASEvaluator, EvaluationConfig, TestDatasetGenerator
from src.inserter import ChromaCollection

# 1. Cargar configuraci√≥n de evaluaci√≥n
eval_config = EvaluationConfig.from_json("configs/evaluation.json")

# 2. Inicializar evaluador (LLM aislado del sistema de generaci√≥n)
evaluator = RAGASEvaluator(eval_config)

# 3. Preparar queries de prueba
generator = TestDatasetGenerator()
test_queries = generator.load_from_json("data/evaluation/test_queries.json")

# 4. Recolectar datos de evaluaci√≥n
collection = ChromaCollection("codeRAG")
queries = [q.question for q in test_queries]
references = [q.reference for q in test_queries]

eval_data = collection.collect_evaluation_data(
    queries=queries,
    references=references,
    verbose=True
)

# 5. Ejecutar evaluaci√≥n
from ragas import EvaluationDataset
dataset = EvaluationDataset.from_list(eval_data)

results = evaluator.evaluate(
    dataset=dataset,
    metrics=["faithfulness", "answer_relevancy"],
    verbose=True
)

# 6. Exportar resultados con versionado
evaluator.export_results(
    results=results,
    output_path="results/my_evaluation.csv",
    format="csv"
)
```

### Estructura de un Dataset de Evaluaci√≥n

**Formato JSON:**
```json
{
  "samples": [
    {
      "question": "¬øC√≥mo funciona la autenticaci√≥n?",
      "reference": "La autenticaci√≥n se maneja mediante...",
      "metadata": {
        "category": "security",
        "difficulty": "medium"
      }
    }
  ]
}
```

**Campos:**
- `question` (requerido): La pregunta a evaluar
- `reference` (opcional): Respuesta de referencia (ground truth)
- `metadata` (opcional): Metadata adicional para an√°lisis

### Configuraci√≥n de Evaluaci√≥n

El archivo `configs/evaluation.json` permite configurar:

```json
{
  "evaluator_llm": {
    "provider": "google",
    "model": "gemini-1.5-flash",
    "temperature": 0.0
  },
  "metrics": ["faithfulness", "answer_relevancy"],
  "batch_size": 10,
  "warn_on_missing_fields": true
}
```

**Par√°metros clave:**
- `evaluator_llm`: Modelo LLM **aislado** para evaluaci√≥n (diferente del modelo de generaci√≥n)
- `metrics`: M√©tricas a calcular
- `warn_on_missing_fields`: Advertir sobre campos faltantes seg√∫n requerimientos de m√©tricas

### Interpretando Resultados

**Rangos de puntuaci√≥n (0.0 - 1.0):**
- **0.8 - 1.0**: Excelente - El sistema est√° funcionando muy bien
- **0.6 - 0.8**: Bueno - Rendimiento aceptable, posibles mejoras
- **0.4 - 0.6**: Regular - Necesita optimizaci√≥n
- **< 0.4**: Pobre - Requiere cambios significativos

**M√©tricas espec√≠ficas para c√≥digo:**
- **Faithfulness alto**: Respuestas basadas en c√≥digo real (sin inventar funciones)
- **Answer Relevancy alto**: Respuestas centradas en la pregunta
- **Context Precision alto**: Fragmentos de c√≥digo recuperados son relevantes
- **Context Recall alto**: Se recuperan todos los fragmentos necesarios

### Generaci√≥n de Referencias (Ground Truth)

**Opci√≥n 1: Auto-generar con RAG**
```python
generator = TestDatasetGenerator()
queries = generator.load_from_json("queries_without_refs.json")

# Genera referencias candidatas
queries_with_refs = generator.build_reference_answers(
    queries=queries,
    collection=collection,
    verbose=True
)

# ‚ö†Ô∏è REVISAR MANUALMENTE las referencias generadas
generator.save_to_json(queries_with_refs, "queries_for_review.json")
```

**Opci√≥n 2: Curaci√≥n interactiva**
```python
generator.curate_references(
    dataset_path="data/evaluation/test_queries.json",
    resume=True  # Saltar queries que ya tienen referencia
)
```

**Opci√≥n 3: Creaci√≥n manual**
Editar directamente el archivo JSON con tus respuestas ideales.

### Versionado y Reproducibilidad

Los resultados exportados incluyen metadata completa para reproducibilidad:
- Versi√≥n de RAGAS, Python, y dependencias
- Modelo de generaci√≥n y evaluaci√≥n usados
- Modelo de embeddings
- Timestamp de evaluaci√≥n
- Snapshot de configuraci√≥n

Ver archivo `.metadata.json` junto al CSV de resultados.

### Buenas Pr√°cticas

1. **Datasets balanceados**: Incluir preguntas de diferentes niveles de dificultad y categor√≠as
2. **Referencias de calidad**: Para m√©tricas que requieren ground truth, asegurar referencias precisas
3. **Evaluaci√≥n peri√≥dica**: Evaluar despu√©s de cambios en configuraci√≥n, modelos, o codebase
4. **Comparaci√≥n de configs**: Usar evaluaci√≥n para comparar diferentes configuraciones (optimal vs fast)
5. **An√°lisis de errores**: Revisar muestras con puntuaci√≥n baja para identificar problemas

### Archivos de Evaluaci√≥n

```
codebase-rag/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.json          # Config de evaluaci√≥n
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îú‚îÄ‚îÄ test_queries.json            # Queries con referencias
‚îÇ       ‚îî‚îÄ‚îÄ code_specific_queries.json   # Queries sin referencias
‚îú‚îÄ‚îÄ results/                     # Resultados de evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results.csv
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_results.metadata.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îú‚îÄ‚îÄ ragas_evaluator.py           # Evaluador principal
‚îÇ       ‚îî‚îÄ‚îÄ test_dataset_generator.py    # Generador de datasets
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_ragas_evaluator.py
‚îî‚îÄ‚îÄ evaluate.py                  # Script CLI de evaluaci√≥n
```

## Mejoras Futuras

- Soporte para modelos de embedding adicionales (opciones locales/open-source)
- Interfaz web para consultas m√°s f√°ciles
- Soporte para actualizaciones incrementales de colecciones existentes
- Filtrado avanzado y b√∫squeda por metadata
- Evaluaci√≥n de rendimiento entre repos con m√∫ltiples lenguajes vs un solo lenguaje
- An√°lisis de code coverage y generaci√≥n de tests
- Soporte multi-modal para diagramas e im√°genes de documentaci√≥n
- **M√©tricas personalizadas de c√≥digo**: Validaci√≥n de sintaxis, correctitud de APIs, alineaci√≥n c√≥digo-documentaci√≥n



### Objetivos Cubiertos

- ‚úÖ Implementaci√≥n de arquitectura RAG completa
- ‚úÖ Integraci√≥n de Data Loading, Text Splitting y Base de Datos Vectorial
- ‚úÖ T√©cnicas de preprocesamiento (detecci√≥n de lenguaje, filtrado, parseo)
- ‚úÖ Dataset de c√≥digo fuente en m√∫ltiples lenguajes
- ‚úÖ Sistema de retrieval con embeddings sem√°nticos

## Licencia

Este proyecto se proporciona tal cual para fines educativos y de desarrollo.
